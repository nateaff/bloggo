---
title: LEGO color themes as topic models
author: Nathanael Aff
date: '2017-09-11'
slug: lego-topic-models
tags: ['Topic Models', 'EDA']
draft: false
showdate: true
---

<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/blazy/blazy.min.js"></script>
<script src="/rmarkdown-libs/pymjs/pym.v1.js"></script>
<script src="/rmarkdown-libs/widgetframe-binding/widgetframe.js"></script>


<p>So I’m back to the LEGO dataset. In a <a href="2017/08/16/exploring-lego-dataset-with-sql-part-ii/">previous post</a>, the plot of the relative frequency of LEGO colors showed that, although there is a wide range of colors on the whole, just a few make up the majority of brick colors. This situation is similar to that encountered with texts, where common words – articles and prepositions, for example – occur frequently but those words’ meaning doesn’t add much to a (statistical) understanding of the text.</p>
<p><img src="/img/plot-relative-1.png" width="600px" height="500px" style="display: block; margin: auto;" /></p>
<p>In this post, I use a few techniques associated with text mining to explore the color themes of LEGO sets. In particular, I’ll build a topic model of LEGO color themes using latent Dirichilet allocation(LDA). Like k-means clustering, the LDA model requires that the user choose the number of topics <span class="math inline">\(k\)</span>. I try out several scoring methods available in R to evaluate the best number of color themes for the LEGO ‘corpus’.</p>
<p><img src="/img/lego-all-colors-1.png" width="600px" height="500px" style="display: block; margin: auto;" /></p>
<div id="note-on-code-and-r-package-shout-outs" class="section level2">
<h2>Note on code and <code>R</code> package shout-outs</h2>
<p>The code for generating the analysis and plots, along with a little math background on the evaluation methods, is in <a href="https://github.com/nateaff/legolda">this repo</a> and this <a href="https://www.kaggle.com/nateaff/finding-lego-color-themes-with-topic-models">Kaggle notebook</a> as the same analysis but run a smaller sample of sets.</p>
<p>One motivation for doing this analysis was to try some methods from the handy <a href="http://tidytextmining.com">Text Mining in R</a> book by Julia Silge and David Robinson and some of my code follows their examples. In particular, in the TF-IDF section and the analysis of the distribution of documents over the topics.</p>
<p>The LDA model is trained using the <code>LDA</code> function from the <code>topicmodels</code> package. Evaluation methods come from <code>ldatuning</code>, <code>SpeedReader</code>, and the <code>clues</code> packages. Unit plots use the <code>waffle</code> package.</p>
</div>
<div id="color-tf-idf" class="section level1">
<h1>Color TF-IDF</h1>
<p>The LEGO dataset contains around 11,000 LEGO sets from 1950 to 2017 and the data includes the part numbers and brick colors that make up each set. Following the text mining analogy, I take LEGO sets to be the ‘documents’ that make up the LEGO ‘corpus’, and colors are the ‘words’ that make up each document or set. I ignored part numbers but it would be possible to incorporate them in a similar analysis by considering the color-part number pair as the unit of meaning.</p>
<p>In text mining, stop words are words that occur with similar frequency in all documents. These words are often removed before performing a statistical analysis. From the frequency plot, it’s clear a few primary colors along with black, gray, and white make up the majority of brick colors. These might have been treated as stop words and removed. Our corups has a vocabulary of just 125 unique colors so I chose to leave all of them in for the analysis.</p>
<p>Term frequency inverse document freqeuncy(TF-IDF) is a metric of a word’s importance to a specific document. Term frequency is the frequency of a word’s appearance in each document and inverse document frequency is the count of documents the word appears in. Weighting TF by IDF means words appearing in all documents are down-weighted compared to rarely occuring words. A high TF-IDF corresponds to a color that is distinct to a LEGO set.</p>
<div id="low-tf-idf-colors" class="section level2">
<h2>Low TF-IDF colors</h2>
<p>First, we’ll look at <em>low</em> TF-IDF colors. Many of the colors-set pairs with a low TF-IDF score show up as common colors in relative frequency plot above.</p>
<div class="figure">
<img src="/img/figure/tf-idf.Rmd/low-tf-idf-plot-1.png" />

</div>
<div id="low-tf-idf-sets" class="section level3">
<h3>Low TF-IDF sets</h3>
<p>The three sets below are the 7-10th lowest TF-IDF set-color combinations. These are large sets with with a common color that appears less frequently in the set than in the corpus. For example, the darker gray in the ‘First LEGO League’ set makes up a small proportion of the set but occurs in many sets.</p>
<div class="figure">
<img src="/img/figure/tf-idf.Rmd/low-tf-idf-plots1-1.png" />

</div>
</div>
</div>
<div id="high-tf-idf-colors" class="section level2">
<h2>High TF-IDF colors</h2>
<p>The plot below shows the 10 set-color pairs with the highest TF-IDF score. These are sets with a <em>high</em> proportion of the set made up of a color that shows up <em>infrequently</em> in LEGO sets overall. The ‘Statue of Liberty’ set is an extreme example; It’s made up almost entirely of a single sea-green color that doesn’t occur in other sets.</p>
<div class="figure">
<img src="/img/figure/tf-idf.Rmd/top-tf-idf-plot-1.png" />

</div>
<div id="high-tf-idf-sets" class="section level3">
<h3>High TF-IDF sets</h3>
<div class="figure">
<img src="/img/figure/tf-idf.Rmd/top-tf-idf-sets-1.png" />

</div>
</div>
</div>
</div>
<div id="building-a-topic-model" class="section level1">
<h1>Building a topic model</h1>
<p>After that somewhat cursory look at the LEGO sets, we’ll move on to building a topic model. The LDA model is a generative model of a body of documents (or LEGO sets or genetic markers or images). The output of the LDA algorithm is two distributions which represent the distribution of terms that make up a topic and the distribution of topics that make up a document. For our model, the <em>term distribution</em> is a distribution over colors that make up a <em>color theme</em>, while the <em>topic distribution</em> is a distribution of <em>color themes</em> that make up a LEGO set.</p>
<p>In this generative model, the LEGO set can be generated by one theme or many themes. Tuning the number of topics in the model changes both the number of themes a set is drawn from and the colors that make up that theme.</p>
</div>
<div id="evaluation-methods" class="section level1">
<h1>Evaluation methods</h1>
<p>I used several methods (chosen because they were readily available in R packages) for evaluating the number of topics that make up the topic model. This is not meant to be an exhaustive list of automated topic evaluation methods. For gauging topic coherence, for example, the Python Gensim library has a more complete pipeline which includes options to modify segmentation, word co-occurence estimation, and scoring methods.</p>
<div id="cross-validation-on-perplexity" class="section level2">
<h2>Cross validation on perplexity</h2>
<p>Perplexity measures the difference between distributions learned on a training and holdout set and the <code>topicmodels</code> package used to build the LDA model has a function for computing perplexity. This was the only evaluation methods which required cross-validation.</p>
<div id="topic-grid" class="section level3">
<h3>Topic grid</h3>
<p>I ended up running the cross-validation twice and refined the spacing on the parameter grid to capture both the larger trend and some detail where I thought better parameters <span class="math inline">\(k\)</span> might be located. There appears to be diminishing returns to model complexity between <span class="math inline">\(k = 20\)</span> and <span class="math inline">\(k = 35\)</span>.</p>
<div class="figure">
<img src="/img/figure/train-model.Rmd/cv-result-plot-1.png" />

</div>
</div>
</div>
<div id="measures-on-the-full-models" class="section level2">
<h2>Measures on the full models</h2>
<p>After running cross-validation on the perplexity scores I reduced the number of models for the remaining evaluation methods. The remaining methods used models trained on the full data set.</p>
</div>
<div id="ldatuning" class="section level2">
<h2>Ldatuning</h2>
<p>The <code>ldatuning</code> package has several other metrics of the quality of the topic models. The skimmed the references in the package documentation but I don’t really understand these measures. At least two of the measures agree that fewer topics are better.</p>
<div class="figure">
<img src="/img/figure/train-model.Rmd/ldatuning-scores-1.png" />

</div>
</div>
<div id="topic-coherence" class="section level2">
<h2>Topic coherence</h2>
<p>There are several versions of topic coherence which measure the pairwise strength of the relationship of the top terms in a topic model. Given some score, where a larger value indicates a stronger relationship between two words <span class="math inline">\(w_i, w_j\)</span>, a generic coherence score is the sum of the top terms in a topic model:</p>
<p><span class="math display">\[ \sum_{w_i, w_j \in W_t} \text{Score}(w_i, w_j), \]</span> with top terms <span class="math inline">\(W_t\)</span> for each topic <span class="math inline">\(t\)</span>.</p>
<p>The coherence score used in the <code>SpeedReader</code> <code>topic_coherence</code> function uses the internal coherence(Umass) of the top terms. I compared the scores for the top 3, 5 and 10 terms.</p>
<div class="figure">
<img src="/img/figure/train-model.Rmd/coherence-score-1.png" />

</div>
</div>
<div id="external-validation-with-cluster-scoring" class="section level2">
<h2>External validation with cluster scoring</h2>
<p>We can also treat the LDA models as a clustering of the LEGO sets by assign each LEGO set to the topic which makes up the highest proportion of that set, that is, the highest probability of a topic <span class="math inline">\(t\)</span> given a document <span class="math inline">\(d\)</span> <span class="math display">\[
\text{gamma} \equiv \gamma = p(t|d).
\]</span> We then assign a document to a cluster by taking <span class="math display">\[
 \text{argmax}_t p(t|d).
\]</span></p>
<p>For comparison’s sake, I also ran a kmeans clustering using the weighted term(color) distribution for each document as the vector that representing that set.</p>
<p>The kmeans and LDA clusterings were evaluated against each sets <code>parent_id</code> label which indicated the theme of the LEGO set. In total there were around 100 unique parent themes although this included sets who were ‘childless parents’.</p>
</div>
<div id="cluster-scores" class="section level2">
<h2>Cluster scores</h2>
<p>The clusters scores include Rand, adjusted Rand, Folkes-Mallow and Jaccard scores. All try to score a clustering on how well the discovered labels match the assigned <code>parent_id</code>. The Rand index assigns a score based on the number pairwise agreement of the cluster labels with the original labels. The other scores use a similar approach and two are versions of the Rand score but adjusted for random agreement. All scores except the un-adjusted Rand index decrease with more topics.</p>
<p>There’s no reason to assume that theme labels match color topics. Poking around the data indicated that some themes ids are closely associated with a palette (Duplo, for example) while other parent themes are general categories with a mix of color theme.</p>
<div class="figure">
<img src="/img/figure/train-model.Rmd/plot-external-scores-1.png" />

</div>
</div>
<div id="topic-distribution" class="section level2">
<h2>Topic Distribution</h2>
<p>The last method for evaluating the learned topics is to look at how documents are distributed over the topic distributions. This example follows a <a href="http://tidytextmining.com/nasa.html#calculating-tf-idf-for-the-description-fields">this section</a> from the tidytext mining book.</p>
<p>The plot below visualizes this as how the <em>documents</em> are distributed over the probability bins for each <em>topic</em>. If too many topics have sets or documents in the low probability bins then you may have too many topics, since few documents are strongly associated with any topic.</p>
<div class="figure">
<img src="/img/figure/train-model.Rmd/plot-40-topics-1.png" />

</div>
<p>The chart above is also closely related to clustering based on LDA. If the distribution over the high probability bins of a topic is sparse then few LEGO sets would be assigned to that topic. (You can compare the plot above to the to the total distribution of LEGO sets over these topics below. Topic 40 had the fewest sets assigned to it.)</p>
</div>
</div>
<div id="evaluation-results" class="section level1">
<h1>Evaluation results</h1>
<p>None of the preceding evaluation methods seem particularly conclusive. The pattern of diminishing returns on the perplexity scores is similar to other model complexity scores and suggests a value for <span class="math inline">\(k\)</span> in the 25-35 range. This agrees somewhat with a visual evaluation of the set distribution over topic probabilities (the last chart above), where at 40 topics some topics seem to have few documents closely associated with them.</p>
</div>
<div id="color-distributions-over-topics" class="section level1">
<h1>Color distributions over topics</h1>
<p>Aside from these scoring methods, we can also plot the color distribution(or relevance) for each topic or color theme directly. Below are charts for the models with 30 and 40 topics.</p>
<p><img src="/img/figure/compare-model-distributions.Rmd/color-distribution-1.png" /><br />
<img src="/img/figure/compare-model-distributions.Rmd/color-distribution-2.png" /></p>
<p>The above plot is based on the beta <span class="math inline">\(\beta\)</span> matrix, which gives the posterior distribution of words given a topic, <span class="math inline">\(p(w|t)\)</span>. The above plot shows a weighted <span class="math inline">\(\beta\)</span> (or relevance) like that used in the <a href="http://www.kennyshirley.com/LDAvis/#topic=0&amp;lambda=0.61&amp;term="><code>LDAvis</code> package</a>.</p>
<p><span class="math display">\[ \text{relevance}(w|t) = \lambda \cdot p(w|t) + (1-\lambda)\cdot \frac{p(w|t)}{p(w)}.\]</span></p>
<div id="how-many-themes" class="section level2">
<h2>How many themes?</h2>
<p>Although there may be more topics that have few sets associated with them as we increase the number of topics, the coherence of a few topics appears to improve.</p>
<p>For example, the two plots below are selections from sets with the highest gamma, <span class="math inline">\(p(d|t)\)</span> for each topic. When we go from 30 to 40 topics the top sets in the topic are removed by the remaining sets are more visually similar. (Also note that the sets that stayed had the top relevance score or weighted <span class="math inline">\(\beta\)</span>).</p>
<div id="topic-2-from-the-30-topic-model" class="section level4">
<h4>Topic 2 from the 30 topic model</h4>
<p><img src="/img/figure/compare-model-distributions.Rmd/topic-waffle-2-2-1.png" /> <br></p>
<div class="kable">
<div id="htmlwidget-1" style="width:100%;height:400px;" class="widgetframe html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"url":"/post/2017-08-21-lego-topic-models_files/figure-html//widgets/widget_topic-view-2-2.html","options":{"xdomain":"*","allowfullscreen":false,"lazyload":false}},"evals":[],"jsHooks":[]}</script>
</div>
<p><br></p>
</div>
<div id="topic-2-from-40-topic-model" class="section level4">
<h4>Topic 2 from 40 topic model</h4>
<div class="figure">
<img src="/img/figure/compare-model-distributions.Rmd/topic-waffle-3-2-1.png" />

</div>
<div class="kable">
<div id="htmlwidget-2" style="width:100%;height:400px;" class="widgetframe html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"url":"/post/2017-08-21-lego-topic-models_files/figure-html//widgets/widget_topic-view-3-2.html","options":{"xdomain":"*","allowfullscreen":false,"lazyload":false}},"evals":[],"jsHooks":[]}</script>
</div>
<p><br></p>
<p>I’ll plot one more topic from the 40 topic model that looked ‘suspicious’ but a sampling of the top sets seem to go well together.</p>
</div>
<div id="topic-32-from-40-topic-model" class="section level4">
<h4>Topic 32 from 40 topic model</h4>
<div class="figure">
<img src="/img/figure/compare-model-distributions.Rmd/topic-waffle-3-32-1.png" />

</div>
<div class="kable">
<div id="htmlwidget-3" style="width:100%;height:400px;" class="widgetframe html-widget"></div>
<script type="application/json" data-for="htmlwidget-3">{"x":{"url":"/post/2017-08-21-lego-topic-models_files/figure-html//widgets/widget_topic-view-3-32.html","options":{"xdomain":"*","allowfullscreen":false,"lazyload":false}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
</div>
<div id="evaluation-summary" class="section level2">
<h2>Evaluation summary</h2>
<p>Of the automated scoring methods, the perplexity scores and the distribution of topics over term probabilities were the only two that seemed readily interpretable and matched my personal sense of which model best identified coherent color themes. I ran the same scores on a small samples of the data for this <a href="https://www.kaggle.com/nateaff/finding-lego-color-themes-with-topic-models">Kaggle notebook</a> and the coherence score consistently pointed at models with fewer topics. It might be interesting to see how evaluation methods vary with parameters of the dataset like vocabulary size, variation in document size and the number of documents.</p>
</div>
</div>
<div id="lego-color-themes" class="section level1">
<h1>LEGO color themes</h1>
<p>For the final model, I used the 40 topic model. Although LDA models the LEGO sets as mixtures of topics or themes, for the next two charts I assigned each set to a single topic using the same method that I used for clustering. And topics are a mixture of colors but I chose a color to represent each topic by blending the topic’s two most important color terms.</p>
<p>In the last plot, the topics are represented by color probabilities of that topic: 1 brick represents roughly 1% of the distribution.</p>
<div class="figure">
<img src="/img/figure/final-model.Rmd/plot-topic-distribution-1.png" />

</div>
<p><br></p>
<div class="figure">
<img src="/img/figure/final-model.Rmd/plot-topic-timeline-1.png" />

</div>
<p><br></p>
<div class="figure">
<img src="/img/figure/lego-color-themes.png" />

</div>
</div>
