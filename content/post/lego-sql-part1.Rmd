---
title: "Exploring the Lego dataset with SQL and dplyr"
author: "Nathanael Aff"
date: 2017-07-25T23:13:14-05:00
categories: ["R", "Statistics"]
tags: ["R", "SQL", "eda"]
---

```{r global_options, echo = FALSE}
knitr::opts_chunk$set(
       fig.align = 'center', 
       echo=TRUE, 
       warning=FALSE, 
       message=FALSE, 
       results = "hide",
       fig.width = 6.5, 
       fig.height = 6.5,
       dev = 'png', 
       eval = FALSE)
```

# Databases and R analysis

There are a number of reasons why using relational databases for your data analyses can be a good practice: doing so requires cleaning and tidying your data, it helps preserve data integrity, and it eases the memory burden of locally analyzing larger datasets. In this tutorial, we will be using the [lego dataset](https://www.kaggle.com/rtatman/lego-database). It's moderately sized at 3 MB and the data comes from a relational database, so it has already been cleaned and [normalized](https://en.wikipedia.org/wiki/Database_normalization). This means we can focus on creating and querying the database.

The main goal of this and a planned follow-up tutorial is to introduce several packages useful for handling SQL databases in R. In this tutorial, we will walk through setting up a database and tables using the `RPostgreSQL` package. In part two we will use the data-manipulation 'verbs' from the `dplyr` package to query and transform the data. 

For the most part, everything will be done from within R and commands should run on any R installation. The major exception is the initial installation of a Postgres database, but I will point to resources for installing and setting up a user in Postgres. The post should be accessible to a beginner to intermediate R user. Some familiarity with SQL syntax and your systems command line might be helpful. 

The lego dataset can be downloaded manually from [Kaggle](https://www.kaggle.com/rtatman/lego-database) with an account. I have mirrored the dataset on S3 to simplify the download step. 

Before getting started install [postgreSQL](https://www.postgresql.org/download/). See the [installation page](https://wiki.postgresql.org/wiki/Detailed_installation_guides) for more detailed instruction and there are more [tutorials here](http://www.postgresqltutorial.com/). Once installed you should have a **user** and **password** with which you can set up a database.

## Setup and download

In addition to `dplyr` and `RPostgreSQL`, we will use the `readr` package was has an enhanced method for reading csv files.

```{r libraries }
# install.packages("dplyr")
# install.packages("RPostgreSQL")
# install.packags("readr")
library(readr)
library(dplyr)
library(RPostgreSQL)
```

Create a project directory -- I'll be calling it 'lego-project' -- and set your current directory to that folder. There are a lot of unhelpful warnings so I'm suppressing those for the session by setting the option `options(warn=-1)`.

```{r setup  }
setwd("~/path/to/my/lego-project")
# Supress warnings for this session
options(warn=-1)
```
We'll download the zip to a temporary file and save the csv files to the data directory.

```{r download  }
URL <- "https://s3-us-west-1.amazonaws.com/kaggle-lego/lego-database.zip"
tmp <- tempfile()
download.file(URL, tmp)
dir.create("data")
# Unzip files in the data directory
setwd("data")
files <- unzip(tmp)
```
## Check out the data

Now we should have the csv files unzipped and the file names in `files`. If you look at the files you'll see it includes 8 csv files and one image named `downloads_schema.png`. This contains the schema for the database the files came from. This details primary and foreign keys and constraints which we won't be using here. The schema will come in handy in the next tutorial, however. 

For now we remove it from the `files` list and grab the filenames for later use. If you have not used the pipe function `%>%`, imported by `dplyr`, it sends output from the left-hand function to the next function and the `.` is a placeholder argument representing that output.

```{r check-files }
# Check files names
files 
# Remove db schema '.png' file
files <- files[-2]
# Remove file prefix and suffix 
filenames <- files %>% sub("./", "", .) %>% sub(".csv", "", .) 
``` 
Here we glance at the first few rows of the data. Several featurs make it a nice alternative to the base `read.csv` function: `read_csv` reports 
the data type of each imported column; It also does not convert strings to factors by default, i.e., `stringsAsFactors = FALSE`. The result of the function is a `tibble` which can be treated as a data.frame. Glancing at the `head()` of the tibble will also report column types. 

```{r glance1 }
# Glance at head of color
readr::read_csv(files[1], n_max = 5)
lapply(files, readr::read_csv, n_max = 5)
```
For a larger dataset, it might also be useful to check the number of rows of data. I'm going to use the `system` function to make a call to `wc`. This is system dependent but should work on Linux or macOS and is an efficient way to get the newline count of a file. I'll turn this into a function so we can use lapply to iterate over the files.

```
# For Linux/Unix or Mac
count_rows <- function(filename){
  system(paste("wc -l", filename))
}
out <- lapply(files, count_rows)
# For Windows (did not test)
count_rows <- function(filename){
  system(paste("nd /c /v 'A String that is extremely unlikely to occur',
        filename))

```
At this point, we have an overview of the data types and size of each file. The data are either integers or strings. One possibility would be to update, for example, the `is_trans` field in colors to be a boolean or logical data type. To keep it simple we will be importing the data types as is. 

## Make the database

Now we have checked out all the ingredients ready can create our database. You'll need a working installation of PostgreSQL and the username and password for an account. The following `db` functions are from the `RPostgreSQL`(and `DBI`) package. 

In this example, we'll use your password as plain text in the script. This isn't the best practice and I give an alternative below. 

First, we identify the database driver we'll be using and setup the connection, `con`, which we can use both create and query the database.

```{r setup-con  }
# Create a connection to the postgresql 
pg = dbDriver("PostgreSQL")
# Note fake user/password
con = dbConnect(pg, 
                user = "my-username", 
               password = "#my-password", 
               host = "localhost", 
               port = 5432)

```
If all has gone well we now have a connection to the Postgres and can create our database. With the `dbSendQuery` command we use any standard PostgreSQL command. We create the database with "create database legos". and then test use the `dbReadTable` to create tables. For each csv file, we create a single table using the `filenames` we created earlier.


```{r create-db  }
dbSendQuery(con, "create database legos")
# Test writing one file to the database
colors <- readr::read_csv(files[1])
dbWriteTable(con, "colors", colors , row.names=FALSE)
# Check that it works
dtab <- dbReadTable(con, "colors")
summary(dtab)
# Write all files to the database
for(k in seq_along(files[1:8])){
  cat("Writing ", filenames[k], "/n")
  tmp <- readr::read_csv(files[k])
  dbWriteTable(con, filenames[k], tmp, row.names=FALSE)  
  rm(tmp)
}
dtab <- dbReadTable(con, "sets")
summary(dtab)
# Close connection
dbDisconnect(con)
```

At this point, our database has been created and we're ready to do some analyses. If we were creating a database which we'd be adding more rows to in the future it would be good to define key constraints or specify our column types in more detail.

## Interlude: Password encryption with `keyringr`

(If you start a new session you can pick up again here. If continuing with the previous analysis, you can skip the code in this section and still run the rest of the tutorial.)

There is a nice alternative to using plain text passwords in scripts. The `keyringr` package lets you encrypt your passwords locally and call those passwords from a script. The package provides three separate functions for doing this and which you should use depends on your operating system. I will use the Linux function below. See [the instructions here](https://cran.r-project.org/web/packages/keyringr/vignettes/Avoiding_plain_text_passwords_in_R_with_keyringr.html) for a quick guide to the other functions. 

We disconnected previously and I assume that we are reconnecting to the database with a fresh R session. 

For the `password` argument passed to the `dbConnect` function below, you will need to replace `keyringr::decrypt_gk_pw("db lego user myusername")` with the `keyringr` decryption function that's appropriate for your operating system. Also, you need to replace the key-value pairs, here `db lego` and `user myusername` with those passed you used to set up your encrypted password.
```{r reconnect}
# If restarting, re-set the directory to data
setwd("~/path/to/my/lego-project")
# Get filenames again
filenames <- dir() %>% sub(".csv", "", .)
install.packages("keyringr")
library(keyringr)
# Replace plaintext password with keyringr call
pg = dbDriver("PostgreSQL")
con = dbConnect(pg, 
                user = "postgres", 
                password = keyringr::decrypt_gk_pw("db lego user myusername"), 
                host = "localhost", 
                port = 5432
                )
```

## Checking the database schema

We retrieved two tables above to verify that they had been created. It would be nice to verify all the tables were created and to check the database's current schema. Since we have a list of the table names, we'll create a function that takes the table names so we can loop over the functions with lapply. 

The [information schema](https://www.postgresql.org/docs/9.1/static/information-schema.html) is a set of views containing information about the database, user roles, data types, etc. We will query the column view to check the data type and maximum character length.  

When `dbSendQuery` does not return data until we call the `dbFetch` function. If you have not fetched the data you'll get an error about 'impending rows'. Clear the list with `dbClearResult(dbListResults(con)[[1]])`. 

```{r echo=FALSE, eval = TRUE, results='hide',message=FALSE}
# Hide re-load
library(dplyr)
library(RPostgreSQL)
setwd("~/dev/R-proj/lego/data")
filenames <- dir() %>% sub(".csv", "", .)

pg = dbDriver("PostgreSQL")
con = dbConnect(pg, 
                user = "postgres", 
                password = keyringr::decrypt_gk_pw("db lego user postgres"), 
                host = "localhost", 
                port = 5432
                )
```


```{r check-schema, eval = TRUE}
# Function to check table schema
get_schema_query <- function(tab){
  paste0("select column_name, data_type, character_maximum_length", 
  " from INFORMATION_SCHEMA.COLUMNS where table_name = '",tab,"';")
}
# Check an example
res <- dbSendQuery(con, get_schema_query("sets"))
dbFetch(res)
dbClearResult(res)
# Create function from previous example
check_schemas <- function(tab){
  res <- dbSendQuery(con, get_schema_query(tab))
  out <- dbFetch(res)
  dbClearResult(res)
  out
}
# Check all tables
schemas <- lapply(filenames, check_schemas)
names(schemas) <- filenames  
```

## Let's plot something 

In the next tutorial, I'll go over using `dplyr` functions to do some basic querying of the data. But we've made it this far so let's plot something. We'll query the color data set and plot all the colors used in lego sets since 1950. 

Colors are stored in the `rgb` column and transparency in `is_trans`. Here we pass `dbSendQuery` a basic SQL select command.

```{r plot-colors , eval = TRUE,  fig.cap="Lego colors, 1950-2017"}
# Select rgb and is_trans columns
res <- dbSendQuery(con, "select rgb, is_trans from colors")
rgb_df <- as_tibble(dbFetch(res))
dbClearResult(res)
# Check data
dim(rgb_df)
head(rgb_df)
```
The data columns have the data type character. The `rgb` colors are in hexadecimal which R can read fine. The values do not reflect the transparency. We use two `dplyr` verbs to count the number of transparent colors. In the next line, we add a column with the color adjusted for transparency. The `ifelse` statement sets the new column, named `rgbt`, to the straight `rgb` value if `is_trans` is false, otherwise, it adds an alpha value determined by the `adjustcolor` function.


```{r transform , eval = TRUE}
rgb_df$rgb <- paste0("#", rgb_df$rgb) 
head(rgb_df)
# Count number of transparent colors
rgb_df %>% filter(is_trans == "t") %>% count()
# And rgb + alpha column
rgb_df <- rgb_df %>% mutate(rgbt = ifelse(is_trans ==  "f", rgb,adjustcolor(rgb, 0.5)))
```
Now we plot the results. The `sort` function orders the hex values alphanumerically, which isn't a perfect color sorting but it's not too bad either. The second version of the plot breaks the colors into two plots to make

```{r  eval = TRUE}
par(bg = "gray10", mar = c(2,1,2,1))
barplot(
        height = rep(0.5, nrow(rgb_df)), 
        col = sort(rgb_df$rgb), 
        yaxt ="n", 
        border = par("bg"),
        space = 0
        )
```

Cool. But what if plot them all with a little more area? We define the function `draw_lego` to draw the single rectangle 

```{r squares, eval = TRUE}
# Draw a single square
draw_lego <- function(col){
  plot(5,5, type = "n", axes = FALSE)
  rect(
        0, 0, 10, 10, 
        type = "n", 
        xlab = "", 
        ylab = "",
        lwd = 4,
        col = col, 
        add = TRUE 
      )
  points(x = 5, y = 5, col ="gray20", cex = 3, lwd = 1.7)
  }

op <- par(bg = "gray12")
plot.new()

par(mfrow = c(9, 15), mar = c(0.7, 0.5, 1, 0.5))
cols <- sort(rgb_df$rgbt)
for(k in 1:length(cols))  draw_lego(cols[k])
```

Maybe a little corny, not perfect, but good enough for now. 

In the follow-up post, I'll go over using `dplyr` to query, filter, and transform data from our lego database. In order to look at more aspects of the data, we'll also need to do some basic joins. I'll put it together to look at how lego colors have changed over time. 

```{r}
dbDisconnect(con)
# Unsupress warnings
options(warn= 0)
```