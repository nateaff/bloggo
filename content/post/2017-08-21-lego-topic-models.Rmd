---
title: Lego Topic Models
author: Nathanael Aff
date: '2017-09-11'
slug: lego-topic-models-rmd
tags: ['Topic Models', 'EDA']
draft: false
showdate: true
---

```{r chunk-options, echo = FALSE}
knitr::opts_chunk$set(
  cache = FALSE,
  comment = NA,
  fig.align = "center",
  tidy = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "100%",
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.align = "center")
```

So I'm back to the Lego dataset. In the [previous post](2017/08/16/exploring-lego-dataset-with-sql-part-ii/) about that dataset, the plot of the relative frequency of unique lego colors showed that a few common colors dominate Lego sets, even though there is a wide range of colors in the Lego sets on the whole. This situation is similar to that encountered with texts, where common words -- articles and prepositions, for example -- occur frequently but those words' meaning doesn't add much to a (statistical) understanding of the text. 

![](/img/plot-relative-1.png)

In this post, I use a few techniques associated with text mining to explore the color themes of Lego sets. In particular, I'll build a topic model of Lego color themes using LDA. Like kmeans clustering, the LDA model requires the user to choose the number of topics $k$. I try out several scoring methods available in R to evaluate the best number of color themes for our Lego sets. 

![](/img/lego-all-colors-1.png) 

## Note on code and `R` package shout-outs  

The code for generating the analysis and plots, along with a little math background on the evaluation methods, is in [this repo](github.com/nateaff/legolda). 

One motivation for doing this analysis was to try some methods from the handy [Text Mining in R](tidytextmining.com) book by Julia Silge and David Robinson and my code follows their examples. In particular in the TF-IDF section and the analysis of the distribution of documents over the topics. The LDA model is trained using the function from the `topicmodels` package which depends on `tm`. Evaluation methods come from `ldatuning`, `SpeedReader`, and the `clues` packages. Unit plots use the `waffle` package. Code for the analysis and results for some of the intermediate computations is available [here](https://nateaff.github.io/legolda/index.html) or you can look at the [Kaggle notebook](kaggle.com/nateaff/finding-lego-color-themes-with-topic-models).


# Color TF-IDF

The Lego dataset contains around 11,000 Lego sets from 1950 to 2017 and the data includes the part numbers and brick colors that make up each set. Following the text mining analogy, I take Lego sets to be the documents that make up the Lego 'corpus' and colors are the 'words' that make up each document or set. (I ignored part numbers.) 

From the frequency plot, it's clear a few primary colors along with black, gray, and white make up the majority of brick colors. In text mining, one might remove some set of stop words that are common to all documents. In our case, there are just 125 unique colors so I chose to use all colors in the analysis. 

TF-IDF is one way to look at which words are more meaningful to each document. Word frequency per document is weighted(inversely) by the number of documents the word occurs in. A high TF-IDF(term frequency $\times$ inverse document frequency) corresponds to a color that most distinctly identifies a Lego set. 

## Low TF-IDF colors

First, we'll look at _low_ TF-IDF colors. Many of the colors are the primary colors, along with black, white and gray, that show up as common colors in relative-frequency plot above. These might be candidates for 'stop colors'. 

![](/img/figure/tf-idf.Rmd/low-tf-idf-plot-1.png)

### Low TF-IDF sets

The three sets below are actually the 7-10th lowest TF-IDF set-color combinations. In common with the other low TF-IDF sets, these are large sets with a somewhat frequent colors that appears less frequently in the set than in the corpus. For example, the darker gray in the `First Lego League` set makes up a small proportion of the set but occurs in many sets.

![](/img/figure/tf-idf.Rmd/low-tf-idf-plots1-1.png)


## High TF-IDF colors

The plot below shows the 10 set-color pairs with the highest TF-IDF score. These are represented by sets with a _high_ proportion of the set made up of a color that shows up _infrequently_ in Lego sets overall. The 'Statue of Liberty' set, for example, is made up almost entirely of a single sea-green color that doesn't occur in other sets. 

![](/img/figure/tf-idf.Rmd/top-tf-idf-plot-1.png)

### High TF-IDF sets

![](/img/figure/tf-idf.Rmd/top-tf-idf-sets-1.png)


# Building a topic model

After that somewhat cursory look at the Lego sets, we'll move on to building a topic model. The latent Dirichlet allocation(LDA) model is a generative model of a body of documents (or Lego sets or genetic markers or images). The output of the LDA algorithm is two distributions which represent the distribution of terms that make up a topic and the distribution of topics that make up a document. For our model, the _term distribution_ is a distribution over _colors_ that make up a _color theme_, while the Lego sets are drawn from a distribution of topics (here color themes). In this model, a Lego set can be generated by one theme or many themes.

# Evaluation methods

I used several methods (chosen because they were readily available in R packages) for evaluating the number of topics that make up the topic model, but this is not meant to be an exhaustive list of automated topic evaluation methods. For gaging topic coherence, for example, the Python Gensim library has a more complete pipeline which includes options to modify segmentation, probability estimation, and scoring methods.

## Cross validation on perplexity 

One method to test how well the learned distributions fit our data is to compare the distribution learned on a training set to the distribution of a holdout set. Perplexity is one measure of the difference between distributions and the `topicmodels` package has a method for computing perplexity. Building topic models is computationally intensive and this was the only evaluation method that required cross-validation.

### Topic grid

I initially ran a test run on a sample set to give me an idea of where I might use a denser grid to check for topic numbers. This wasn't the greatest idea, as I found out later, since using a smaller data set tended to shift the perplexity scores leftward indicating fewer topics were ideal. I ended up running the cross-validation twice and refined the spacing on the parameter grid to capture both the larger trend and some detail where I thought better parameter $k$ was located.


The smoothed fit (in black) appears to bend around 25-30 and it would have been nice to have another evaluation at 25. 

![](/img/figure/train-model.Rmd/cv-result-plot-1.png )   

## Measures on the full models

After running cross-validation on the perplexity scores I reduced the number of models for the remaining evaluation methods. The remaining methods used models trained on the full data set.

## Ldatuning

The `ldatuning` package has several other metrics of the quality of the topic models. The skimmed the references in the package documentation but I don't really understand these measures. At least two of the measures agree that fewer topics are better.

![](/img/figure/train-model.Rmd/ldatuning-scores-1.png)

## Topic coherence

There are several versions of topic coherence which measure the pairwise strength of the relationship of the top terms in a topic model. Given some score, where a larger value indicates a stronger relationship between two words $w_i, w_j$, a generic coherence score is the sum of the top terms in a topic model: 

 $$ \sum_{w_i, w_j \in W_t} \text{Score}(w_i, w_j), $$
 with top terms $W_t$ for each topic $t$.
 
The coherence score used in the `SpeedReader` `topic_coherence` function uses the internal coherence(Umass) of the top terms. I compared the scores for the top 3, 5 and 10 terms. 

![](/img/figure/train-model.Rmd/coherence-score-1.png)   

## External validation with cluster scoring 

We can also treat the LDA models as a clustering of the Lego sets by assign each Lego set to the topic which makes up the highest proportion of that set, that is, the highest probability of a topic $t$ given a document $d$
\[
\text{gamma} \equiv \gamma = p(t|d).
\]
We then assign a document to a cluster by taking
\[
 \text{argmax}_t p(t|d).
\]

For comparison's sake, I also ran a kmeans clustering using the weighted term(color) distribution for each document as the vector that representing that set.

The kmeans and LDA clusterings were evaluated against each sets `parent_id` label which indicated the theme of the Lego set. In total there were around 100 unique parent themes although this included sets who were 'childless parents'. 

## Cluster scores

The clusters scores include Rand, adjusted Rand, Folkes-Mallow and Jaccard scores. All try to score a clustering on how well the discovered labels match the assigned `parent_id`. The Rand index assigns a score based on the number pairwise agreement of the cluster labels with the original labels. The other scores use a similar approach and two are versions of the Rand score but adjusted for random agreement. All scores except the un-adjusted Rand index decrease with more topics.

There's no reason to assume that theme labels match color topics. Poking around the data indicated that some themes ids are closely associated with a palette (Duplo, for example) while other parent themes are general categories with a mix of color theme.

![](/img/figure/train-model.Rmd/plot-external-scores-1.png)

## Topic Distribution 

The last method for evaluating the learned topics is to look at how documents are distributed over the topic distributions. This example follows a [this section](http://tidytextmining.com/nasa.html#calculating-tf-idf-for-the-description-fields) from the tidytext mining book. 


The plot below visualizes this as how the _documents_ are distributed over the probability bins for each _topic_. If too many topics have sets or documents in the low probability bins then you may have too many topics, since few documents are strongly associated with any topic. 

![](/img/figure/train-model.Rmd/plot-40-topics-1.png)

The chart above is also closely related to clustering based on LDA. If the distribution over the high probability bins of a topic is sparse then few Lego sets would be assigned to that topic. (You can compare the plot above to the to the total distribution of Lego sets over these topics below. Topic 40 had the fewest sets assigned to it.)

# Evaluation results

None of the preceding evaluation methods seem particularly conclusive. The pattern of diminishing returns on the perplexity scores is similar to other model complexity scores and suggests a value for $k$ in the 25-35 range. This agrees somewhat with a visual evaluation of the set distribution over topic probabilities (the last chart above), where at 40 topics some topics seem to have few documents closely associated with them.  

# Color distributions over topics

Aside from these scoring methods, we can also plot the color distribution(or relevance) for each topic or color theme directly. Below are charts for the models with 30 and 40 topics.  

![](/img/figure/compare-model-distributions.Rmd/color-distribution-1.png)  
![](/img/figure/compare-model-distributions.Rmd/color-distribution-2.png)  

The above plot is based on the beta $\beta$ matrix, which gives the posterior distribution of words given a topic, $p(w|t)$. The above plot shows a weighted $\beta$ (or relevance) like that used in the [`LDAvis` package](http://www.kennyshirley.com/LDAvis/#topic=0&lambda=0.61&term=).


\[ \text{relevance}(w|t) = \lambda \cdot p(w|t) + (1-\lambda)\cdot \frac{p(w|t)}{p(w)}.\]

## How many themes?

Although there may be more topics that have few sets associated with them as we increase the number of topics, the coherence of a few topics appears to improve.

For example, the two plots below are selections from sets with the highest gamma, $p(d|t)$ for each topic. When we go from 30 to 40 topics the top sets in the topic are removed by the remaining sets are more visually similar. (Also note that the sets that stayed had the top relevance score or weighted $\beta$).

#### Topic 2 from the 30 topic model 

![](/img/figure/compare-model-distributions.Rmd/topic-waffle-2-2-1.png)
<br>

<div class = "kable"> 


```{r topic-view-2-2}
library(kableExtra)
  options(knitr.table.format = "html")

view_topic <-readRDS("view-topic-2-2.RDS")

# Style caption
caption <- htmltools::tags$caption(
             style = paste0(
                      'caption-side: top;',  
                      'text-align: left;'),  
           "Sets most associated with topic 2 from the 30 topic model")

dt1 <- DT::datatable(view_topic, 
       # Change column names
            height = 400, 
            caption = caption) %>%
       # Format columns 
       DT::formatStyle(1:9, `font-size`= '12px', `font-family`= 'Lato')

# Use widgetframe package to render table
widgetframe::frameWidget(dt1)
 
```
</div>
<br>

#### Topic 2 from 40 topic model

![](/img/figure/compare-model-distributions.Rmd/topic-waffle-3-2-1.png)

<div class = "kable"> 

```{r topic-view-3-2}
view_topic <- readRDS("view-topic-3-2.RDS")

# Style caption
caption <- htmltools::tags$caption(
             style = paste0(
                      'caption-side: top;',  
                      'text-align: left;'),  
           "Sets most associated with topic 2 from the 40 topic model")

dt1 <- DT::datatable(view_topic, 
       # Change column names
            height = 400, 
            caption = caption) %>%
       # Format columns 
       DT::formatStyle(1:9, `font-size`= '12px', `font-family`= 'Lato')

# Use widgetframe package to render table
widgetframe::frameWidget(dt1)
 
```
</div>
<br>

I'll plot one more topic from the 40 topic model that looked 'suspicious' but a sampling of the top sets seem to go well together. 

#### Topic 32 from 40 topic model 

![](/img/figure/compare-model-distributions.Rmd/topic-waffle-3-32-1.png)

<div class = "kable"> 

```{r topic-view-3-32}
view_topic <- readRDS("view-topic-32-3.RDS")

# Style caption
caption <- htmltools::tags$caption(
             style = paste0(
                      'caption-side: top;',  
                      'text-align: left;'),  
           "Sets most associated with topic 32 from the 40 topic model")

dt1 <- DT::datatable(view_topic, 
       # Change column names
            height = 400, 
            caption = caption) %>%
       # Format columns 
       DT::formatStyle(1:9, `font-size`= '12px', `font-family`= 'Lato')

# Use widgetframe package to render table
widgetframe::frameWidget(dt1)
 
```
</div>

## Evaluation summary 

Of the automated scoring methods, the perplexity scores and the distribution of topics over term probabilities were the only two that seemed readily interpretable and matched my personal sense of which model best identified coherent color themes. 

# Lego color themes

For the final model, I used the 40 topic model. Although LDA models the Lego sets as mixtures of topics or themes, for the next two charts I assigned each set to a single topic using the same method that I used for clustering. And topics are a mixture of colors but I chose a color to represent each topic by blending the topic's two most important color terms.  

In the last plot, the topics are represented by color probabilities of that topic: 1 brick represents roughly 1% of the distribution.

![](/img/figure/final-model.Rmd/plot-topic-distribution-1.png)

![](/img/figure/final-model.Rmd/plot-topic-timeline-1.png)

![](/img/figure/color-themes-big.png)







